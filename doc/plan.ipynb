{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Plan:\n",
    "## Assumptions:\n",
    "- I am going to assume that the recommender engine will not include user/customer information. You would need user demographics, user behavior, and probably device data and other data to make a recommender, but we don't have that. I assume that will be what is being simulated by the data scientists. Because I don't know what variables they will want to change, I don't know what fields they will need in the tables, so for now I am just going to assume they will be creating user demographic data.\n",
    "- I am going to assume that the data scientists can use pandas.\n",
    "- I am going to assume that analysts can use notebooks or pgadmin to interact with the db.\n",
    "- I am going to assume that the data scientists do not want to use a notebook to interact with the db. I am going to assume that they want to use python scripts to interact with the db.\n",
    "\n",
    "## Overall Objectives:\n",
    "- Provide a database to support the simulation engine.\n",
    "- Offer both direct SQL access and Python APIs for read and write operations.\n",
    "- Create a demonstration recommender engine.\n",
    "\n",
    "## Plan:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Evaluate and Select RDBMS:\n",
    "#### Objective: Choose an RDBMS that fits the needs of the project.\n",
    "##### PostgreSQL is an good choice for the recommender system for these reasons:\n",
    "- Scalability: It offers high scalability, accommodating both vertical and horizontal growth, making it suitable for an expanding system.\n",
    "- Performance: PostgreSQL is known for handling complex queries efficiently. This is particularly beneficial for a recommender system that requires complex data interactions.\n",
    "- Community Support: With a strong open-source community, PostgreSQL provides extensive documentation, forums, and support, aiding in both development and troubleshooting.\n",
    "- Ease of Integration with Python ORMs: PostgreSQL can be easily interfaced with Python Object-Relational Mapping (ORM) tools like FastAPI and SQLModel, allowing for a more streamlined and efficient development process.\n",
    "- In summary, the combination of scalability, advanced query performance, vibrant community support, and easy integration with popular Python ORMs makes PostgreSQL a fitting choice for building a recommender system that can evolve and adapt to complex data needs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Create raw schema for raw original data ✅"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Load raw data into raw schema in db to meet analyst request to see raw data via ad hoc queries.\n",
    "1. ✅ Load into db (see load_raw_data.ipynb)\n",
    "2. ✅ Create read only user for analysts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Design and Normalize the Database:\n",
    "#### Objective: Transform the initial data into a suitable data model.\n",
    "#### Tasks:\n",
    "- ✅ Analyze initial data.\n",
    "- ✅ Design tables and relationships.\n",
    "- ✅ Normalize to at least 2NF to balance analytical needs and performance\n",
    "- ✅ Document assumptions and rationale for design choices.\n",
    "\n",
    "#### Assumptions and rationale for schema design choices: \n",
    "(see ERD here: artifacts/recommender_db_erd_2023-08-31.png)\n",
    "\n",
    "1. I have designed this as a snowflaked fact/dimension table, with the fact_sessions table and the dim_users table being fabricate (with some fabricated data loaded for demo purposes). It is advisable to structure the data at the lowest grain level possible (a session of content consumption), so that analysis can be as fine-grained as users might want, and can then be aggregated for reports or feature engineering.\n",
    "2. The fact_sessions table has only quantitative data, other than foreign keys which connect to the dimension tables. This allows us to add dim tables or add fields to dim tables without having to change the fact table.\n",
    "3. This schema preserves all the information that was in the original raw tables, but because the data is more normalized, fewer tables are needed. The boolean fields, \"is_year_best\", \"is_all_time_best\", \"is_main_genre\", and \"is_main_country\" capture all new information from the \"best\" raw tables.\n",
    "4. It is highly debatable whether it is optimal to snowflake out dim_genres, dim_prod_coutries, and dim_credits, instead of leaving them denormalized in dim_titles. This is a compromise between analytical vs. simulation creation ease. If, after discussion with the data team, scientists plan on using these tables for analysis, then I would denormalize these back into the dim_title table. If they are going to be creating different simpulations of data for each specific table, then I may leave them normalized, allowing for the scientists to have to make fewer updates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Create relational schema for normalized data, create tables schemas, load data into tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1. Set Up Database, Load Initial Data, Fabricate Session and User Data:\n",
    "##### Tasks:\n",
    "- ✅ Set up the selected RDBMS. \n",
    "- ✅ Create tables as designed.\n",
    "- ✅ Load initial data. \n",
    "- ✅ Test with sample queries to ensure everything is working. \n",
    "- ✅ Create user with read/write access to the database.\n",
    "- ✅ Fabricate session and user data for demo purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Enable interaction with the database through Python APIs.\n",
    "#### Tasks:\n",
    "- ✅ Implement read and write operations with SQL\n",
    "- ✅ Create web api for read and write operations. (still needs update operations)\n",
    "- ✅ Ensure security and validation of inputs. Done in web api\n",
    "- ✅ Create Python APIs for read and write operations.\n",
    "- ✅ Create testing for local python api (still needs error case testing)\n",
    "- ✅ Create testing for web api (still needs error case testing)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Build a Demo Recommender Engine:\n",
    "#### Objective: Showcase the functionality of the APIs and database.\n",
    "#### Tasks:\n",
    "- ✅ Develop a simple recommender engine.\n",
    "- ✅ Implement reading and writing data using the developed APIs.\n",
    "- ✅ Test with real or simulated user data.\n",
    "\n",
    "- 5.1 Develop a Simple Recommender Engine\n",
    "Choose a recommendation algorithm that fits the scope of your demonstration. Collaborative filtering is a common approach that can be implemented relatively quickly.\n",
    "You can use libraries like Scikit-Surprise, which offers various recommendation algorithms.\n",
    "\n",
    "- 5.2 Implement Reading and Writing Data Using APIs\n",
    "Within your recommender engine, make HTTP requests to the APIs you developed in Part 4 to read and write data.\n",
    "For reading, you might retrieve user preferences, historical ratings, or other relevant information.\n",
    "For writing, you might store predictions or user feedback.\n",
    "\n",
    "- 5.3 Test with Real or Simulated User Data\n",
    "Create tests that mimic real user interactions, or use a dataset that resembles what real users might provide.\n",
    "Ensure that the recommender engine can make reasonable predictions and that the read and write operations function correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Provide Improvement Suggestions:\n",
    "#### Objective: Analyze the solution and propose improvements.\n",
    "#### Tasks:\n",
    "- ✅ Review the entire solution.\n",
    "- ✅ Identify areas for potential improvements.\n",
    "- ✅ Document suggestions.\n",
    "\n",
    "##### Ideas for improvements:\n",
    "- Generally make the database and apis more robust and secure.\n",
    "    - Dockerize the database and apis. Or, if not dockerize the database, then migrate the database to a cloud service.\n",
    "    - Add error handling to local api\n",
    "    - Complete testing, with error case testing, for all apis.\n",
    "- If this system were going to break, how would it break? Consider a systematic way to discover where in the code each change would have ramifications. This involves defining likely error cases very specifically, improving code in relevant areas, and then testing for them.\n",
    "    - A change to db could easily cause the connections to break or user permissions to go haywire.\n",
    "    - A change to any of the models would have ramifications in any of the apis that use them. \n",
    "    - Change to file/folder structure\n",
    "    - Haven't done enough error testing to see, but passing the wrong data type to the apis could cause problems.\n",
    "    - Third party module changes or other dependencies could cause problems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus Challenges (Optional):\n",
    "✅ Write unit and integration tests for the solution. (partially complete)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
